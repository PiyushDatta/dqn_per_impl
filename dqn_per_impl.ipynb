{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "  def __init__(self, num_states: int, num_actions: int, hidden_layer_size: int) -> None:\n",
    "    \"\"\"\n",
    "      DQN using mlp\n",
    "    \"\"\"\n",
    "    super(Model, self).__init__()\n",
    "\n",
    "    self.layer1 = torch.nn.Sequential(\n",
    "        torch.nn.Linear(num_states, hidden_layer_size),\n",
    "        torch.nn.BatchNorm1d(hidden_layer_size),\n",
    "        torch.nn.PReLU()\n",
    "    )\n",
    "\n",
    "    self.layer2 = torch.nn.Sequential(\n",
    "        torch.nn.Linear(hidden_layer_size, hidden_layer_size),\n",
    "        torch.nn.BatchNorm1d(hidden_layer_size),\n",
    "        torch.nn.PReLU()\n",
    "    )\n",
    "\n",
    "    self.final_layer = torch.nn.Linear(hidden_layer_size, num_actions)\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.final_layer(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple, NamedTuple\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "  def __init__(self, capacity: int) -> None:\n",
    "    self.capacity = capacity\n",
    "    self.sum_tree = Sumtree(capacity=capacity)\n",
    "    self.transition = namedtuple(\"Transition\",\n",
    "                                 field_names=[\"prev_state\", \"action\",\n",
    "                                              \"reward\", \"curr_state\",\n",
    "                                              \"done\"])\n",
    "    # Hyperparameter that we use to avoid some experiences\n",
    "    # to have 0 probability of being taken\n",
    "    self.err = 0.01\n",
    "    # Hyperparameter that we use to make a tradeoff between\n",
    "    # taking only exp with high priority and sampling randomly\n",
    "    self.alpha = 0.6\n",
    "    # importance-sampling, from initial value increasing to 1\n",
    "    self.beta = 0.4\n",
    "    self.beta_increment_per_sampling = 0.001\n",
    "    # clipped abs error\n",
    "    self.absolute_error_upper = 1.0\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return self.sum_tree.n_entries\n",
    "\n",
    "  def _get_priority(self, abs_err: np.float) -> float:\n",
    "    abs_errs = np.abs(abs_err) + self.err\n",
    "    clipped_errors = np.minimum(abs_errs, self.absolute_error_upper).item()\n",
    "    return clipped_errors ** self.alpha\n",
    "\n",
    "  def update(self, tree_idx: int, abs_err: np.float) -> None:\n",
    "    priority = self._get_priority(abs_err=abs_err)\n",
    "    self.sum_tree.update(tree_idx=tree_idx, priority=priority)\n",
    "\n",
    "  def push(self, prev_state: np.ndarray, action: int,\n",
    "           reward: int, curr_state: np.ndarray, done: bool) -> None:\n",
    "    # Find the max priority (from the leaves)\n",
    "    max_priority = np.max(self.sum_tree.tree[-self.sum_tree.capacity:]).item()\n",
    "\n",
    "    # if the max priority = 0 we can't put priority = 0\n",
    "    # since this exp will never have a chance to be selected\n",
    "    # so we use a minimum priority\n",
    "    if max_priority == 0:\n",
    "      max_priority = self.absolute_error_upper\n",
    "\n",
    "    new_exp = self.transition(prev_state, action, reward, curr_state, done)\n",
    "    self.sum_tree.add(max_priority, new_exp)\n",
    "\n",
    "  def sample(self, batch_size: int) -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "      return a sample of with length of batch size.\n",
    "      batch contains transitions (experience).\n",
    "      indices and is_weights are lists of floats.\n",
    "    \"\"\"\n",
    "    batch = []\n",
    "    indices = []\n",
    "    priorities = []\n",
    "\n",
    "    priority_segment = self.sum_tree.total_priority / batch_size\n",
    "    self.beta = np.min([1.0, self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      a = priority_segment * i\n",
    "      b = priority_segment * (i+1)\n",
    "      uniform_sample = random.uniform(a, b)\n",
    "\n",
    "      idx, priority, data = self.sum_tree.get(uniform_sample)\n",
    "      indices.append(idx)\n",
    "      priorities.append(priority)\n",
    "      batch.append(data)\n",
    "\n",
    "    # p(j)\n",
    "    sampling_probabilities = priorities / self.sum_tree.total_priority\n",
    "    # IS = (1/N * 1/P(i))**b /max wi == (N*P(i))**-b  /max wi\n",
    "    is_weights = np.power(self.sum_tree.n_entries * sampling_probabilities,\n",
    "                          -self.beta)\n",
    "    is_weights /= is_weights.max()\n",
    "\n",
    "    return batch, indices, is_weights.tolist()\n",
    "\n",
    "\n",
    "class Sumtree(object):\n",
    "  def __init__(self, capacity: int) -> None:\n",
    "    \"\"\"\n",
    "      A binary tree data structure where the value of a node is equal \n",
    "      to sum of the nodes present in its left subtree and right subtree.\n",
    "    \"\"\"\n",
    "    self.capacity = capacity\n",
    "    self.n_entries = 0\n",
    "    self.data_pointer = 0\n",
    "\n",
    "    # remember we are in a binary node (each node has max 2 children)\n",
    "    # so 2x size of leaf (capacity) - 1 (root node)\n",
    "    self.tree = np.zeros(2*capacity-1)\n",
    "    # contains the experiences (so the size of data is capacity)\n",
    "    self.data = np.zeros(capacity, dtype=object)\n",
    "\n",
    "  def add(self, priority: int, new_data: NamedTuple) -> None:\n",
    "    \"\"\"\n",
    "      store the priority and experience\n",
    "    \"\"\"\n",
    "    tree_idx = self.data_pointer + self.capacity - 1\n",
    "    self.data[self.data_pointer] = new_data\n",
    "    self.update(tree_idx, priority)\n",
    "\n",
    "    self.data_pointer += 1\n",
    "    # If we're above the capacity, you go back to first index (we overwrite)\n",
    "    if self.data_pointer >= self.capacity:\n",
    "      self.data_pointer = 0\n",
    "\n",
    "    if self.n_entries < self.capacity:\n",
    "      self.n_entries += 1\n",
    "\n",
    "  def update(self, tree_idx: int, priority: float) -> None:\n",
    "    \"\"\"\n",
    "      update the priority\n",
    "    \"\"\"\n",
    "    change = priority - self.tree[tree_idx]\n",
    "    self.tree[tree_idx] = priority\n",
    "\n",
    "    # update the parent nodes\n",
    "    while True:\n",
    "      tree_idx = (tree_idx-1) // 2\n",
    "      self.tree[tree_idx] += change\n",
    "      if tree_idx == 0:\n",
    "        break\n",
    "\n",
    "  def get(self, value: float) -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "      get the leaf index, priority value of that leaf, and experience associated with that index\n",
    "    \"\"\"\n",
    "    parent_idx = 0\n",
    "    left_child_idx = 2 * parent_idx + 1\n",
    "    right_child_idx = left_child_idx + 1\n",
    "\n",
    "    while left_child_idx < len(self.tree):\n",
    "      # downward search, always search for a higher priority node\n",
    "      if value < self.tree[left_child_idx]:\n",
    "        parent_idx = left_child_idx\n",
    "      else:\n",
    "        value -= self.tree[left_child_idx]\n",
    "        parent_idx = right_child_idx\n",
    "\n",
    "      left_child_idx = 2 * parent_idx + 1\n",
    "      right_child_idx = left_child_idx + 1\n",
    "\n",
    "    data_idx = parent_idx - self.capacity + 1\n",
    "    return parent_idx, self.tree[parent_idx], self.data[data_idx]\n",
    "\n",
    "  @property\n",
    "  def total_priority(self) -> np.float64:\n",
    "    \"\"\"\n",
    "      find the sum of nodes by returning the root node\n",
    "    \"\"\"\n",
    "    return self.tree[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from gym.envs.classic_control import CartPoleEnv\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class Agent():\n",
    "  def __init__(self, env: gym.envs.classic_control.CartPoleEnv, debug: bool, checkpoint_path: str,\n",
    "               hidden_layer_size: int, replay_memory_cap: int, batch_size: int,\n",
    "               learning_rate: float, learning_rate_decay: float, discount_factor: float) -> None:\n",
    "    \"\"\"\n",
    "      Agent class that is responsible for training our neural network and overall \n",
    "      managing the DQN.\n",
    "    \"\"\"\n",
    "    self.env = env\n",
    "    self.num_states = env.observation_space.shape[0]\n",
    "    self.num_actions = env.action_space.n\n",
    "\n",
    "    self.debug = debug\n",
    "\n",
    "    # if gpu is to be used\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.device(self.device)\n",
    "    # if debug:\n",
    "    print(\"Using device: %s\" % self.device)\n",
    "\n",
    "    self.__model = Model(num_states=self.num_states, num_actions=self.num_actions,\n",
    "                         hidden_layer_size=hidden_layer_size).to(self.device)\n",
    "\n",
    "    self.learning_rate_decay = learning_rate_decay\n",
    "    self.optimizer = torch.optim.Adam(\n",
    "        self.__model.parameters(), lr=learning_rate)\n",
    "    self.__scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer=self.optimizer, gamma=learning_rate_decay)\n",
    "    self.loss_function = torch.nn.MSELoss()\n",
    "    self.checkpoint_path = checkpoint_path\n",
    "    self.discount_factor = discount_factor\n",
    "\n",
    "    self.replay_memory = ReplayMemory(replay_memory_cap)\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  @property\n",
    "  def model(self) -> Model:\n",
    "    return self.__model\n",
    "\n",
    "  def preprocess_observation(self, observation: np.ndarray) -> torch.Tensor:\n",
    "    return torch.autograd.Variable(torch.Tensor(observation).to(self.device))\n",
    "\n",
    "  def predict(self, input_data: np.ndarray) -> torch.Tensor:\n",
    "    processed_data = self.preprocess_observation(\n",
    "        input_data.reshape(-1, self.num_states))\n",
    "    self.__model.train(mode=False)\n",
    "    return self.__model(processed_data)\n",
    "\n",
    "  def get_action(self, observation: np.ndarray, epsilon: float) -> int:\n",
    "    \"\"\"\n",
    "      Only 2 actions, left and right. \n",
    "      0 = move black box thing left.\n",
    "      1 = move black box thing right\n",
    "\n",
    "      args:\n",
    "        observation: List of floats. List[cart position, cart velocity, pole angle, pole velocity at tip].\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "      final_action = self.env.action_space.sample()\n",
    "    else:\n",
    "      self.__model.train(mode=False)\n",
    "      scores = self.predict(observation)\n",
    "      _, max_val_idx = torch.max(scores.cpu().data, 1)\n",
    "      final_action = int(max_val_idx.numpy())\n",
    "\n",
    "    return final_action\n",
    "\n",
    "  def decay_learning_rate(self) -> None:\n",
    "    if len(self.replay_memory) >= self.batch_size and self.learning_rate_decay > 0.00:\n",
    "      self.__scheduler.step()\n",
    "\n",
    "  def get_last_lr(self) -> float:\n",
    "    return self.__scheduler.get_last_lr()[0]\n",
    "\n",
    "  def save_weights(self) -> None:\n",
    "    # make the file if not exists, torch.save doesn't work without existing file\n",
    "    try:\n",
    "      if not os.path.exists(self.checkpoint_path):\n",
    "        if not os.path.isdir(os.path.dirname(self.checkpoint_path)):\n",
    "          os.mkdir(os.path.dirname(self.checkpoint_path))\n",
    "        with open(self.checkpoint_path, 'w+'):\n",
    "          pass\n",
    "\n",
    "      if self.debug:\n",
    "        print(\"Saving weights to: \" + str(self.checkpoint_path))\n",
    "\n",
    "      torch.save(self.__model.state_dict(), self.checkpoint_path)\n",
    "    except Exception as e:\n",
    "      print(\"Could not save weights to: \" + str(self.checkpoint_path))\n",
    "      print(\"ERROR: %s\" % e)\n",
    "\n",
    "  def load_weights(self, name: str = '') -> None:\n",
    "    try:\n",
    "      self.__model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "      if self.debug:\n",
    "        print(\"Loaded weights for \" + name +\n",
    "              \", from: \" + str(self.checkpoint_path))\n",
    "    except Exception as e:\n",
    "      print(\"Could not load weights for \" + name +\n",
    "            \", from: \" + str(self.checkpoint_path))\n",
    "      print(\"ERROR: %s\" % e)\n",
    "\n",
    "  def copy_weights(self, agent_to_copy: 'Agent') -> None:\n",
    "    self.__model.load_state_dict(agent_to_copy.model.state_dict())\n",
    "\n",
    "  def add_experience(self, prev_state: np.ndarray, action: int,\n",
    "                     reward: int, curr_state: np.ndarray, done: bool) -> None:\n",
    "    self.replay_memory.push(prev_state, action, reward, curr_state, done)\n",
    "\n",
    "  def train(self, target_agent: 'Agent') -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "      Train on a single game. Only train if our replay memory has enough saved memory, \n",
    "      which should be >= batch size.\n",
    "\n",
    "      We take a minibatch (of size batch_size) from our replay memory. We use our \n",
    "      train_agent (policy network) to predict the Q values for the previous states, \n",
    "      and take the Q-value of the action taken in prev_state (we can do this using \n",
    "      our actions batch). We use our target_agent (target network) to predict the \n",
    "      max Q values for the current states, but we use these Q values from target_agent \n",
    "      in our bellman equation to get the max Q values. Finally, we compare the Q \n",
    "      values from the policy network with the Q value we get from the bellman equation.\n",
    "    \"\"\"\n",
    "    # only start training process when we have enough experiences in the replay\n",
    "    if len(self.replay_memory) < self.batch_size:\n",
    "      return 0.00, 0.00, 0.00\n",
    "\n",
    "    # sample random batch from replay memory\n",
    "    (minibatch,\n",
    "     minibatch_indices,\n",
    "     minibatch_IS_weight) = self.replay_memory.sample(self.batch_size)\n",
    "\n",
    "    prev_states = np.vstack([x.prev_state for x in minibatch])\n",
    "    actions = torch.LongTensor(\n",
    "        np.array([x.action for x in minibatch]).reshape(-1, 1)).to(self.device)\n",
    "    rewards = torch.FloatTensor(\n",
    "        np.array([x.reward for x in minibatch]).reshape(-1, 1)).to(self.device)\n",
    "    curr_states = np.vstack([x.curr_state for x in minibatch])\n",
    "    dones = torch.FloatTensor(\n",
    "        np.array([x.done for x in minibatch]).reshape(-1, 1)).to(self.device)\n",
    "\n",
    "    # use train network to predict q values of prior states (before actual states)\n",
    "    # get the q value of action taken for the prior state (given by actions)\n",
    "    q_predict = self.predict(prev_states).gather(1, actions)\n",
    "\n",
    "    # use bellman equation to get expected q-value of actual states\n",
    "    # we get the max q value here, regardless of action taken for prior state\n",
    "    q_curr_state_values = target_agent.predict(\n",
    "        curr_states).max(dim=1, keepdim=True)[0].detach()\n",
    "    mask = 1 - dones\n",
    "    q_target = (rewards + self.discount_factor *\n",
    "                q_curr_state_values * mask).to(self.device)\n",
    "\n",
    "    # get the absolute error between predict and target\n",
    "    # and update our replay memory\n",
    "    errors = torch.abs(q_predict - q_target).cpu().data.numpy()\n",
    "    for i in range(self.batch_size):\n",
    "      self.replay_memory.update(minibatch_indices[i], errors[i])\n",
    "\n",
    "    # train our network based on the results from its\n",
    "    # q_predict to expected values given by our target network (q_target)\n",
    "    self.__model.train(mode=True)\n",
    "    self.optimizer.zero_grad()\n",
    "    loss = (torch.FloatTensor(minibatch_IS_weight).to(self.device)\n",
    "            * self.loss_function(q_predict, q_target)).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping (use if you want)\n",
    "    # for param in self.__model.parameters():\n",
    "    #   param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "    self.optimizer.step()\n",
    "\n",
    "    # convert the loss and q_target to floats and\n",
    "    # return them so we can analyze later\n",
    "    float_loss = np.mean(loss.cpu().detach().numpy()).item()\n",
    "    float_q_target = np.mean(q_target.cpu().detach().numpy()).item()\n",
    "    float_err = np.mean(errors).item()\n",
    "    return float_loss, float_q_target, float_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from gym.envs.classic_control import CartPoleEnv\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Dict\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "def train_agent(env: gym.envs.classic_control.CartPoleEnv, train_agent: Agent, target_agent: Agent,\n",
    "                progress_print_per_iter: int, total_episodes: int, episode_epsilon: float,\n",
    "                min_epsilon: float, max_epsilon_episodes: int, epsilon_decay: float,\n",
    "                copy_max_count: int, saved_results_path: str, saved_results_name: str,\n",
    "                hyperparams_dict: Dict) -> None:\n",
    "  \"\"\"\n",
    "    Train the agent on a number of games/episodes.\n",
    "    Decay epsilon by epsilon_decay if given, otherwise decay using max_epsilon_episodes.\n",
    "    Decay learning rate by learning_rate_decay if present, otherwise don't decay.\n",
    "    Record and plot data on matplotlib and also save the figues/numbers.\n",
    "  \"\"\"\n",
    "  total_rewards = 0\n",
    "  total_steps = 0\n",
    "  total_loss = 0.0\n",
    "  total_bellman_eq = 0.0\n",
    "  total_errs = 0.0\n",
    "  avg_reward = deque(maxlen=100)\n",
    "  progress_bar = tqdm(total=total_episodes)\n",
    "  solved_game = False\n",
    "\n",
    "  plotting_data = {\n",
    "      'avg_rewards[last_%s]' % avg_reward.maxlen: np.empty(total_episodes),\n",
    "      'total_rewards': np.empty(total_episodes),\n",
    "      'epsilon': np.empty(total_episodes),\n",
    "      'loss': np.empty(total_episodes),\n",
    "      'bellman_eq': np.empty(total_episodes),\n",
    "      'errors': np.empty(total_episodes),\n",
    "      'learning_rate': np.empty(total_episodes),\n",
    "  }\n",
    "\n",
    "  for episode in range(total_episodes):\n",
    "    # epsilon decay\n",
    "    episode_epsilon = epsilon_decay_formula(episode=episode, max_episode=max_epsilon_episodes,\n",
    "                                            min_epsilon=min_epsilon, epsilon=episode_epsilon,\n",
    "                                            epsilon_decay=epsilon_decay)\n",
    "\n",
    "    # train game/episode, save weights, decay learning rate\n",
    "    (total_rewards, total_steps,\n",
    "     total_loss, total_bellman_eq,\n",
    "     total_errs) = train_single_game(env=env,\n",
    "                                     train_agent=train_agent,\n",
    "                                     target_agent=target_agent,\n",
    "                                     epsilon=episode_epsilon,\n",
    "                                     copy_max_count=copy_max_count,\n",
    "                                     total_steps=total_steps)\n",
    "    train_agent.decay_learning_rate()\n",
    "    train_agent.save_weights()\n",
    "\n",
    "    # update matplotlib data\n",
    "    avg_reward.append(total_rewards)\n",
    "    plotting_data['avg_rewards[last_%s]' %\n",
    "                  avg_reward.maxlen][episode] = np.mean(avg_reward)\n",
    "    plotting_data['total_rewards'][episode] = total_rewards\n",
    "    plotting_data['epsilon'][episode] = episode_epsilon\n",
    "    plotting_data['loss'][episode] = total_loss\n",
    "    plotting_data['bellman_eq'][episode] = total_bellman_eq\n",
    "    plotting_data['errors'][episode] = total_errs\n",
    "    plotting_data['learning_rate'][episode] = train_agent.get_last_lr()\n",
    "\n",
    "    # update progress bar\n",
    "    if ((episode+1) % progress_print_per_iter) == 0:\n",
    "      progress_bar.update(progress_print_per_iter)\n",
    "      progress_bar.set_postfix({\n",
    "          'episode reward': total_rewards,\n",
    "          'avg reward (last %s)' % avg_reward.maxlen: np.mean(avg_reward),\n",
    "          'epsilon': episode_epsilon,\n",
    "      })\n",
    "    \n",
    "    if not solved_game and np.mean(avg_reward) >= 195:\n",
    "      solved_game = True\n",
    "      print(\"Solved in %s games/episodes\" % (episode+1))\n",
    "\n",
    "  env.close()\n",
    "  save_results(plotting_data=plotting_data, progress_bar=progress_bar,\n",
    "               name=saved_results_name, directory_name=saved_results_path,\n",
    "               hyperparams_dict=hyperparams_dict)\n",
    "\n",
    "\n",
    "def train_single_game(env: gym.envs.classic_control.CartPoleEnv,\n",
    "                      train_agent: Agent, target_agent: Agent,\n",
    "                      epsilon: float, copy_max_count: int,\n",
    "                      total_steps: int) -> Tuple[int, int, float, float, float]:\n",
    "  \"\"\"\n",
    "    Train the agent on one game/episode.\n",
    "    Update target agent model weights to the same as train agent's after some number of steps.\n",
    "    Return the total rewards given by the environment, and the loss given by the agent/model.\n",
    "\n",
    "    observation := [cart position, cart velocity, pole angle, pole velocity at tip]\n",
    "  \"\"\"\n",
    "  prev_observation = env.reset()\n",
    "  observation = None\n",
    "  total_rewards = 0\n",
    "  total_loss = 0\n",
    "  avg_bellman_eq = 0\n",
    "  total_bellman_eq = 0\n",
    "  total_errs = 0\n",
    "  reward, game_done = None, False\n",
    "\n",
    "  while not game_done:\n",
    "    # Get our agent's action and record the environment\n",
    "    action = train_agent.get_action(\n",
    "        observation=prev_observation, epsilon=epsilon)\n",
    "    observation, reward, game_done, _ = env.step(action)\n",
    "    total_rewards += reward\n",
    "    total_steps += 1\n",
    "\n",
    "    if game_done:\n",
    "      reward -= 1\n",
    "\n",
    "    # Add the observations we got from the environment\n",
    "    train_agent.add_experience(prev_observation, action, reward,\n",
    "                               observation, game_done)\n",
    "    # Get the loss and bellman equation values from training\n",
    "    total_loss, avg_bellman_eq, total_errs = train_agent.train(target_agent)\n",
    "    total_bellman_eq = avg_bellman_eq if avg_bellman_eq > 0 else total_bellman_eq\n",
    "    # adjust prev state to curr state for next iteration\n",
    "    prev_observation = observation\n",
    "\n",
    "    # copy weights of policy net to our target net after a certain amount of steps\n",
    "    if total_steps % copy_max_count == 0:\n",
    "      target_agent.copy_weights(train_agent)\n",
    "\n",
    "  return total_rewards, total_steps, total_loss, total_bellman_eq, total_errs\n",
    "\n",
    "\n",
    "def save_results(plotting_data: Dict[str, np.ndarray], progress_bar: tqdm,\n",
    "                 name: str, directory_name: str, hyperparams_dict: Dict) -> None:\n",
    "  \"\"\"\n",
    "    Save the progress bar and the matplotlib figures to a directory.\n",
    "  \"\"\"\n",
    "  # create all the necessary directories\n",
    "  parent_dir = os.path.abspath(os.path.join(directory_name, os.pardir))\n",
    "  if not os.path.exists(parent_dir):\n",
    "    os.mkdir(parent_dir)\n",
    "  if not os.path.exists(directory_name):\n",
    "    os.mkdir(directory_name)\n",
    "  directory_name = directory_name + \"/\" + name\n",
    "\n",
    "  # plot all our data\n",
    "  def plot_figure(data: np.ndarray, xlabel: str, ylabel: str, save_path: str) -> None:\n",
    "    plt.clf()\n",
    "    plt.plot(data)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "  for name, plot_data in plotting_data.items():\n",
    "    plot_figure(data=plot_data, xlabel='Episode', ylabel=name,\n",
    "                save_path=directory_name + name + '.png')\n",
    "\n",
    "  # Save the hyperparams and progress bar in a text file\n",
    "  with open(directory_name+'pbar.txt', 'w', encoding=\"utf-8\") as filetowrite:\n",
    "    filetowrite.write(\"==== Hyperparams: ====\\n\")\n",
    "    for key, val in hyperparams_dict.items():\n",
    "      filetowrite.write(key + \": %s\" % val)\n",
    "      filetowrite.write(\"\\n\")\n",
    "    filetowrite.write(\"\\n\")\n",
    "    filetowrite.write(str(progress_bar))\n",
    "\n",
    "\n",
    "def play_game(env: gym.envs.classic_control.CartPoleEnv, agent: Agent,\n",
    "              epsilon: float, game_render: bool = False) -> None:\n",
    "  \"\"\"\n",
    "    Play a single game.\n",
    "\n",
    "    observation := [cart position, cart velocity, pole angle, pole velocity at tip]\n",
    "  \"\"\"\n",
    "  observation = env.reset()\n",
    "  done = False\n",
    "  total_episodes = 0\n",
    "\n",
    "  while not done:\n",
    "    if game_render:\n",
    "      env.render()\n",
    "    action = agent.get_action(observation=observation, epsilon=epsilon)\n",
    "    observation, _, done, _ = env.step(action)\n",
    "    total_episodes += 1\n",
    "\n",
    "  env.close()\n",
    "  print(\"\\nTotal rewards/time steps: {0}.\".format(total_episodes))\n",
    "\n",
    "\n",
    "def epsilon_decay_formula(episode: int, max_episode: int, min_epsilon: float,\n",
    "                          epsilon: float, epsilon_decay: float) -> float:\n",
    "  \"\"\"\n",
    "    If there is an epsilon decay value, then we use that.\n",
    "\n",
    "    Otherwise use max_epsilon_episodes, which will look like the graph below:\n",
    "    Returns ðœº-greedy\n",
    "    1.0---|\\\n",
    "          | \\\n",
    "          |  \\\n",
    "    min_e +---+------->\n",
    "              |\n",
    "              max_episode\n",
    "  \"\"\"\n",
    "  if epsilon_decay > 0:\n",
    "    new_epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "  else:\n",
    "    slope = (min_epsilon - 1.0) / max_episode\n",
    "    new_epsilon = max(min_epsilon, slope * episode + epsilon)\n",
    "\n",
    "  return new_epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "curr_try = 1\n",
    "hyperparams_dict = {\n",
    "    # total eps, printing, and memory\n",
    "    'total_episodes':  500,\n",
    "    'replay_memory_cap': 50000,\n",
    "    'progress_per_iteration': 10,\n",
    "\n",
    "    # learning rate\n",
    "    'learning_rate':  0.001,\n",
    "    'learning_rate_decay': 0.00,\n",
    "    # 'learning_rate_decay':  0.001,\n",
    "\n",
    "    # epsilon\n",
    "    'epsilon': 1.0,\n",
    "    'max_epsilon_episodes': 50,\n",
    "    'min_epsilon': 0.01,\n",
    "    'epsilon_decay': 0.00,\n",
    "    # 'epsilon_decay': 0.995,\n",
    "\n",
    "    # other factors\n",
    "    'discount_factor': 0.99,\n",
    "    'batch_size': 32,\n",
    "    'copy_max_step': 25,\n",
    "    'hidden_layer_size': 48,\n",
    "\n",
    "    # weights save path\n",
    "    'checkpoint_path': os.path.join(os.path.join(os.getcwd(), 'nn_saved_weights'), 'training_%s.pth' % curr_try),\n",
    "\n",
    "    # results save path\n",
    "    'saved_results_path': os.path.join(os.path.join(os.getcwd(), 'saved_results'), 'training_%s' % curr_try),\n",
    "\n",
    "    # results save name\n",
    "    'saved_results_name': '',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from gym.envs.classic_control import CartPoleEnv\n",
    "\n",
    "\n",
    "def print_all_hyperparams() -> None:\n",
    "  \"\"\"\n",
    "    Print all the hyper parameters.\n",
    "  \"\"\"\n",
    "  print()\n",
    "  print(\"==== Hyperparams: ====\")\n",
    "  for key, val in hyperparams_dict.items():\n",
    "    print(key + \": %s\" % val)\n",
    "  print()\n",
    "\n",
    "\n",
    "def get_agent(env: gym.envs.classic_control.CartPoleEnv, agent_debug: bool) -> Agent:\n",
    "  \"\"\"\n",
    "    Returns Agent class.\n",
    "  \"\"\"\n",
    "  return Agent(env=env,\n",
    "               debug=agent_debug,\n",
    "               checkpoint_path=str(hyperparams_dict['checkpoint_path']),\n",
    "               hidden_layer_size=int(hyperparams_dict['hidden_layer_size']),\n",
    "               batch_size=int(hyperparams_dict['batch_size']),\n",
    "               learning_rate=float(hyperparams_dict['learning_rate']),\n",
    "               learning_rate_decay=float(\n",
    "                   hyperparams_dict['learning_rate_decay']),\n",
    "               discount_factor=float(hyperparams_dict['discount_factor']),\n",
    "               replay_memory_cap=int(hyperparams_dict['replay_memory_cap']))\n",
    "\n",
    "\n",
    "def main(agent_debug: bool = False, train_model: bool = False) -> None:\n",
    "  \"\"\"\n",
    "    Train on multiple episodes or play a single game.\n",
    "\n",
    "    observation space (input size) = 4 --> env.observation_space.shape[0]\n",
    "    action space (output size) = 2 --> env.action_space.n\n",
    "  \"\"\"\n",
    "  my_env = gym.make('CartPole-v0')\n",
    "  my_train_agent = get_agent(env=my_env, agent_debug=False)\n",
    "  my_target_agent = get_agent(env=my_env, agent_debug=False)\n",
    "\n",
    "  if os.path.exists(str(hyperparams_dict['checkpoint_path'])):\n",
    "    my_train_agent.load_weights(name=\"training agent\")\n",
    "    my_target_agent.load_weights(name=\"target agent\")\n",
    "\n",
    "  if train_model:\n",
    "    print_all_hyperparams()\n",
    "    train_agent(env=my_env, train_agent=my_train_agent, target_agent=my_target_agent,\n",
    "                progress_print_per_iter=int(\n",
    "                    hyperparams_dict['progress_per_iteration']),\n",
    "                total_episodes=int(hyperparams_dict['total_episodes']),\n",
    "                episode_epsilon=float(hyperparams_dict['epsilon']),\n",
    "                min_epsilon=float(hyperparams_dict['min_epsilon']),\n",
    "                max_epsilon_episodes=int(\n",
    "                    hyperparams_dict['max_epsilon_episodes']),\n",
    "                epsilon_decay=float(hyperparams_dict['epsilon_decay']),\n",
    "                copy_max_count=int(hyperparams_dict['copy_max_step']),\n",
    "                saved_results_path=str(hyperparams_dict['saved_results_path']),\n",
    "                saved_results_name=str(hyperparams_dict['saved_results_name']),\n",
    "                hyperparams_dict=hyperparams_dict)\n",
    "  else:\n",
    "    play_game(env=my_env, agent=my_train_agent, epsilon=0.00)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  gym.logger.set_level(40)\n",
    "  # main(train_model=True)\n",
    "  main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36764bit4be6ff81bd7c4e408047b6ed755667e9",
   "display_name": "Python 3.6.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}